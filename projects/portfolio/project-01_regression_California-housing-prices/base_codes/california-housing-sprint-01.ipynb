{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worldwide-gambling",
   "metadata": {},
   "source": [
    "### **D2APR: Aprendizado de M√°quina e Reconhecimento de Padr√µes** (IFSP, Campinas) <br/>\n",
    "**Prof**: Samuel Martins (Samuka) <br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-complexity",
   "metadata": {},
   "source": [
    "#### Custom CSS style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-frequency",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".dashed-box {\n",
    "    border: 1px dashed black !important;\n",
    "#    font-size: var(--jp-content-font-size1) !important;\n",
    "}\n",
    "\n",
    ".dashed-box table {\n",
    "\n",
    "}\n",
    "\n",
    ".dashed-box tr {\n",
    "    background-color: white !important;\n",
    "}\n",
    "        \n",
    ".alt-tab {\n",
    "    background-color: black;\n",
    "    color: #ffc351;\n",
    "    padding: 4px;\n",
    "    font-size: 1em;\n",
    "    font-weight: bold;\n",
    "    font-family: monospace;\n",
    "}\n",
    "// add your CSS styling here\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-pollution",
   "metadata": {},
   "source": [
    "<span style='font-size: 2.5em'><b>California Housing üè°</b></span><br/>\n",
    "<span style='font-size: 1.5em'>Predict the median housing price in California districts</span>\n",
    "\n",
    "<span style=\"background-color: #ffc351; padding: 4px; font-size: 1em;\"><b>Sprint #1</b></span>\n",
    "\n",
    "<img src=\"./imgs/california-flag.png\" width=300/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-salad",
   "metadata": {},
   "source": [
    "## Before starting this notebook\n",
    "This jupyter notebook is designed for **experimental and teaching purposes**. <br/>\n",
    "Although it is (relatively) well organized, it aims at solving the _target problem_ by evaluating (and documenting) _different solutions_ for somes steps of the **machine learning pipeline** ‚Äî see the ***Machine Learning Project Checklist by xavecoding***. <br/>\n",
    "We tried to make this notebook as literally a _notebook_. Thus, it contains notes, drafts, comments, etc.<br/>\n",
    "\n",
    "For teaching purposes, some parts of the notebook may be _overcommented_. Moreover, to simulate a real development scenario, we will divide our solution and experiments into **\"sprints\"** in which each sprint has some goals (e.g., perform _feature selection_, train more ML models, ...). <br/>\n",
    "The **sprint goal** will be stated at the beginning of the notebook.\n",
    "\n",
    "A ***final notebook*** (or any other kind of presentation) that compiles and summarizes all sprints ‚Äî the target problem, solutions, and findings ‚Äî should be created later.\n",
    "\n",
    "#### Conventions\n",
    "\n",
    "<ul>\n",
    "    <li>üí° indicates a tip. </li>\n",
    "    <li> ‚ö†Ô∏è indicates a warning message. </li>\n",
    "    <li><span class='alt-tab'>alt tab</span> indicates and an extra content (<i>e.g.</i>, slides) to explain a given concept.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-answer",
   "metadata": {},
   "source": [
    "## üéØ Sprint Goals\n",
    "- Frame the problem\n",
    "- Get the data\n",
    "- Data cleaning\n",
    "- Simple EDA to gain insights\n",
    "- Initial data preprocessing\n",
    "- Train a (single) ML algorithm with all features and default hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-shelf",
   "metadata": {},
   "source": [
    "### 0. Imports and default settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-watts",
   "metadata": {},
   "source": [
    "## üî≤ 1. Frame the Problem\n",
    "\n",
    "### üìÑ 1.1. Context\n",
    "<table align=\"left\" class='dashed-box'>\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>If you still do not have the context of the problem or if it is uncler to you, you may search for the subject on the internet, for example, on news sites or academic papers.</td>\n",
    "</tr>\n",
    "<tr style=\"background-color: white !important\">\n",
    "    <td></td>\n",
    "    <td>This will help you not only to understand the problem better, but also in formulating and writing its context here.</td>\n",
    "</tr>\n",
    "</table><br/><br/><br/><br/>\n",
    "    \n",
    "The  housing  market  is  an  important,  yet  special,  sector  of  the  national  economy.  Its  importance  is  derived  from  two  main  factors:  the  size  of  the  housing  market  and  the  functions  that  residential  properties  perform.  Residential properties additionally perform a number of important socio-economic functions. They meet the most basic needs of having a place to call home and a sense of security, as well as the needs of a higher order, such as the possibility of forming social ties or meeting the needs of self-realization [1].\n",
    "\n",
    "**California** is the most populous state in the United States (U.S.) with a population about 39.78 million (July, 2020) [2]. It also the largest economy in U.S., boasting a $3.2 trillion gross state product (GSP) as of 2019. [3]. If California were a sovereign nation (2020), it would rank as the world's fifth largest economy [4].\n",
    "\n",
    "California has the most expensive and largest housing markets in the U.S. [5]. Apart from having one of the highest numbers of migrant workers, California also holds the highest real estate prices. The average price of a home in California is approximately 2.5 times the median national price [5]. It even surpasses the housing prices of Massachusetts and New York, the two states with the most expensive homes and rental rates [5]. <br/><br/>\n",
    "\n",
    "\n",
    "**References:** <br/>\n",
    "[1] Zelazowski, K.. \"Housing market cycles in the context of business cycles.\" Real Estate Management and Valuation 25, no. 3 (2017): 5-14. <br/>\n",
    "[2] State of California, Department of Finances - https://www.dof.ca.gov/Forecasting/Demographics/Estimates/E-2/ (accessed on 2021-08-19). <br/>\n",
    "[3] Gross Domestic Product by State, Fourth Quarter and Annual 2019\" (PDF). US Department of Commerce, BEA (Bureau of Economic Analysis). <br/>\n",
    "[4] \"Report for Selected Countries and Subjects\". International Monetary Fund - https://www.imf.org/en/Publications/SPROLLs/world-economic-outlook-databases  <br/>\n",
    "[5] 21 Eye-Opening California Real Estate Statistics - https://movity.com/blog/california-real-estate-statistics (accessed on 2021-08-19) <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-campus",
   "metadata": {},
   "source": [
    "### üß† 1.2. Challenge\n",
    "ML California Housing is a well-established real estate company working on the housing market in California.\n",
    "\n",
    "#### üéØ **Objective:**\n",
    "**Build a machine learning solution to automatically predict the median housing prices in the _districts_ (block groups) of California.** <br/>\n",
    "These predictions will be used to determine whether it is worth investing in a given area or not.\n",
    "\n",
    "#### **Baseline:**\n",
    "Currently, the **district housing prices** are estimated ***manually by experts***: a team gathers up-to-date information about a district and finds out the _median housing price_. \n",
    "This is _costly_ and _time-consuming_, and their **estimates are not great**; they often realize that **their estimates were off by more than 20%**.\n",
    "\n",
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>We will consider a dataset with housing prices of 1990 for learning purposes (see <i>Section 2. Get the Data</i>).</td>\n",
    "</tr>\n",
    "</table><br/><br/>\n",
    "\n",
    "#### **Solution Planning:**\n",
    "- **Regression problem**\n",
    "- Metrics:\n",
    "    - R¬≤\n",
    "    - Root Mean Squared Error (RMSE)\n",
    "- Data sources:\n",
    "    - [1990 California Housing Prices](https://github.com/ageron/handson-ml2/blob/master/datasets/housing/README.md)\n",
    "- No assumptions\n",
    "- Project deliverable:\n",
    "    - A simple exploratory data analysis\n",
    "    - **A ML system/model** launched in _production_ <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-timeline",
   "metadata": {},
   "source": [
    "## üíΩ 2. Get the Data\n",
    "The considered dataset contains housing prices from the _1990 California census_. It is a dataset provided by the book _\"Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts, tools, and techniques to build intelligent systems (A. G√©ron, 2019)\"_ which was adapted from the StatLib repository for teaching purposes.\n",
    "\n",
    "This dataset is not recent and **should not** be used to predict current California housing prices; more recent data should be used instead. However, it is an excellent one for learning and simulating a real case.\n",
    "\n",
    "We refer to the [github repository](https://github.com/ageron/handson-ml2/tree/master/datasets/housing) for details about the changes of the dataset.\n",
    "\n",
    "This dataset does not have legal obligations and sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-spoke",
   "metadata": {},
   "source": [
    "### 2.1. Download the Data\n",
    "We previously download the dataset from this [github repository](https://github.com/ageron/handson-ml2/blob/master/datasets/housing/housing.csv). However, we could use pandas to read such a remote file directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-focus",
   "metadata": {},
   "source": [
    "### 2.2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "housing = pd.read_csv('./datasets/housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-bacteria",
   "metadata": {},
   "source": [
    "### 2.3. Take a quick look at the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-accountability",
   "metadata": {},
   "source": [
    "Each row corresponds to a _block group of houses_, called here _\"district\"_ for short, <br/>\n",
    "Each _district_ is represented by **10 attributes** (9 numeric and 1 categorical). <br/>\n",
    "- District's location: **_longitude_, _latitude_, _ocean_proximity_**\n",
    "- **_housing_median_age_**: Median age of a house within the district (a lower number is a newer building)\n",
    "- **_population_**: Total number of people residing within the district..\n",
    "- **_households_**: Total number of households (a group of people residing within a home unit) of the district\n",
    "- **_total_rooms_** and **_total_bedrooms_**: Total number of rooms and bedrooms, respectively, of all home units from the district.\n",
    "- **_median_income_**: Median income for households within the district (measured in tens of thousands of US Dollars)\n",
    "- **_median_house_value_**: Median value/price for houses within the ditrict (measured in US Dollars)\n",
    "\n",
    "\n",
    "There are **20,640 instances** in the dataset (fairly small by ML standards). <br/>\n",
    "The attribute _total_bedrooms_ has only **20,433 *nonnull* values**, meaning that _207 districts_ are missing this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram for each numeric attibute from the dataframe\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "display()  # just to avoid texts in the notebook output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-belize",
   "metadata": {},
   "source": [
    "1. The _median income attribute_ indeed is not expressed in US dollars (USD). After checking with the team that collected the data, you are told that the numbers represent roughly _tens of thousands_ of dollars (_e.g._, 3 actually means about $30,000). <br/>\n",
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem.</td>\n",
    "</tr>\n",
    "</table><br/><br/><br/>\n",
    "\n",
    "2. The _housing median age_ and _the median house value_ were **capped**. <br/>\n",
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>Your Machine Learning algorithms may learn that prices never go beyond that limit.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>You need to check with your client team to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>a. Collect proper values for the districts whose prices were capped.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>b. Remove those districts from the training and testing sets.</td>\n",
    "</tr>\n",
    "</table><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "3. These attributes have very _different scales_.\n",
    "4. Many histograms are _tail-heavy_. This may make it a bit harder for some ML algorithms to detect patterns.\n",
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>We will try transforming these attributes in later sprints to have more bell-shaped distributions.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-poverty",
   "metadata": {},
   "source": [
    "### 2.4. Create a **`Test Set`**\n",
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td><span class='alt-tab'>alt-tab</span></td>\n",
    "    <td>Splitting a Dataset: Hold-out, random sampling, stratified sampling</td>\n",
    "</tr>\n",
    "</table><br/><br/>\n",
    "\n",
    "_Splitting/sampling_ the dataset into a _training set_ and a ***testing set*** (also called _hold out set_) is crucial for developing of ML solutions. You train your solutions using the _training set_, and you test it using the _testing set_. <br/>\n",
    "The error rate on new cases is called the **generalization error** (or out-of-sample error), and by evaluating your model on the test set, you get an _estimate_ of this error. <br/>\n",
    "This value tells you _how well_ your model will perform on instances it has _never seen before_.\n",
    "\n",
    "Many authors and machine learning practitioners **do not** perform this task _at this stage_ of the ML pipeline. <br/>\n",
    "They typically use the **entire dataset** for data cleaning, preprocessing, and exploratory data analysis.\n",
    "\n",
    "\n",
    "For example, when using the **entire dataset** to _impute_ missing values from a specific attribute with its median, one will be _snooping/looking at_ the _future test samples_. <br/>\n",
    "Consequently, your estimate for the generalization error will be **biased**, and you will launch a system (in production) that _may not_ perform as well as expected. <br/>\n",
    "This is called ***data snooping bias***. <br/>\n",
    "Therefore, performing dataset sampling at this stage of the ML pipeline is recommended. <br/>\n",
    "\n",
    "**OBSERVATIONS:**<br/>\n",
    "However, there are _some concerns_ with that.\n",
    "\n",
    "Suppose you have a dataset with 100 samples. <br/>\n",
    "Initially, you randomly divide it into a **training set** (80 samples) and a **testing set** (20 samples). <br/>\n",
    "Next, during _data cleaning_, you found out that the _training set_ has **5 samples with missing values** for a given attribute and **5 duplicated samples**. <br/>\n",
    "Similarly, the _testing set_ has **8 samples with missing values** for the same attribute and **2 duplicated samples**. <br/>\n",
    "You then decided **to remove _all_ these samples**, resulting in:\n",
    "- training set with 70 samples\n",
    "- testing set with 10 samples.\n",
    "\n",
    "The dataset has changed (80 samples) and, consequently, the _initial proportion_ of the train and test sets too: _80%_ to ___87.5%___ (70/80), and _20%_ to ___12.5%___ (10/80). <br/>\n",
    "This can impact model training and/or assessment in the _testing set_. <br/>\n",
    "\n",
    "A _reasonable strategy_ is to perform any _data cleaning tasks_ that **remove samples** -- _e.g._, duplicated samples, samples with missing values, outliers, etc -- across the _entire dataset_ **before** splitting it into. <br/>\n",
    "\n",
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>It is common to use 80% of the data for training and 20% for testing. However, this depends on the size of the dataset: if it contains 10 million instances, then holding out 1% means your test set will contain 100,000 instances, probably more than enough to get a good estimate of the generalization error.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-commander",
   "metadata": {},
   "source": [
    "### Checking for duplicated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.duplicated().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-roberts",
   "metadata": {},
   "source": [
    "There are no duplicated samples (rows) in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-saturday",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>In some cases, we could have a given attribute (column) whose <i>values</i> should be <b>unique</b>; e.g., ID.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>Then, one should check if there are <i>multiple samples</i> with <i>the same value</i> ‚Äî regardless of their other attributes ‚Äî and decide how to remove the duplicity.</td>\n",
    "</tr>\n",
    "</table><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-heavy",
   "metadata": {},
   "source": [
    "### Checking samples with missing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-calendar",
   "metadata": {},
   "source": [
    "As we identified before (Section 2.3), only the attribute _total_bedrooms_ has missing values. Our strategy here will be _to impute_ a given value (e.g., the median) instead of dropping the samples. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-anatomy",
   "metadata": {},
   "source": [
    "### Segmenting samples by their `median income`\n",
    "\n",
    "We will follow the premise from the book that \"the _median income_ is a very important attribute to predict _median housing prices_\". So, we need to guarantee that training and testing set are _representative_ for that, otherwise we will have ***bias selection***. <br/>\n",
    "One way to do that is _to group_ the samples according to their _median income_ and perform a **stratified sampling**.\n",
    "\n",
    "Since we do not have a categorical variable for the _median income_, we can segment the samples by creating groups (_strata_) according to ranges of _median incomes_. <br/>\n",
    "Let's then check the distribution of the _median income_ to decide the upper and lower bounds of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-newport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "romantic-contamination",
   "metadata": {},
   "source": [
    "Most median income values are clustered around 1.5 to 6 (i.e., \\\\$15,000 - \\\\$60,000), but some median incomes go far beyond 6.\n",
    "\n",
    "It is important to have a _sufficient number of instances_ in your dataset for each group (_stratum_), or else the estimate of a stratum‚Äôs importance may be biased. <br/>\n",
    "This means that you **should not have too many strata**, and **each stratum should be *large enough***.\n",
    "\n",
    "We follow the book and consider five intervals to group the samples:\n",
    "- Group 1: [0, 1.5]\n",
    "- Group 2: (1.5, 3]\n",
    "- Group 3: (3, 4.5]\n",
    "- Group 3: (4.5, 6]\n",
    "- Group 4: (6, +inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-myrtle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['median_income_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportional of the groups in the entire dataset\n",
    "housing['median_income_group'].value_counts() / len(housing['median_income_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(housing['median_income_group'])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-funeral",
   "metadata": {},
   "source": [
    "### Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-johns",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of the groups in the training set\n",
    "housing_train['median_income_group'].value_counts() / len(housing_train['median_income_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of the groups in the testing set\n",
    "housing_test['median_income_group'].value_counts() / len(housing_test['median_income_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-residence",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>In <i><b>classification problems,</b></i> it is common to have an <b>imbalance</b> between the classes. If this imbalance is <b>severe</b>, the trained classifier may be <i>biased</i> to the largest classes, even performing <i>stratified sampling</i> and choosing suitable metrics for this scenario.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>This can also happen in <i><b>regression problems</b></i> like this one when considering a <i>categorical variable</i> to sample the dataset by stratified sampling.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>One possible solution is to perform some data sampling method tailored for imbalanced datasets. See more <a href=\"https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/\">here</a>.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the attribute 'median_income_group'\n",
    "housing_train = housing_train.drop(columns=['median_income_group'])\n",
    "housing_test = housing_test.drop(columns=['median_income_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-completion",
   "metadata": {},
   "source": [
    "#### **Saving datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to keep the reference for the row indices from the original dataset, remove the index=False\n",
    "housing_train.to_csv('./datasets/housing_train_sprint-1.csv', index=False)\n",
    "housing_test.to_csv('./datasets/housing_test_sprint-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-attack",
   "metadata": {},
   "source": [
    "## üßπ 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-roulette",
   "metadata": {},
   "source": [
    "### 3.1. Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-america",
   "metadata": {},
   "source": [
    "We checked earlier that the `total_bedrooms` attribute has some missing values, so let‚Äôs fix this. You have three options:\n",
    "    \n",
    "1. Get rid of the corresponding districts. <br/>\n",
    "<code>housing_train.dropna(subset=[\"total_bedrooms\"])</code>\n",
    "\n",
    "2. Get rid of the whole attribute. <br/>\n",
    "<code>housing_train.drop(\"total_bedrooms\", axis=1)</code>\n",
    "\n",
    "3. Set the values to some value (zero, the mean, the median, etc.). <br/>\n",
    "<code>median_total_bedrooms = housing_train[\"total_bedrooms\"].median()\n",
    "housing_train[\"total_bedrooms\"].fillna(median_total_bedrooms, inplace=True)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-interview",
   "metadata": {},
   "source": [
    "To avoid dropping the samples with missing `total_bedrooms`, we will choose *option 3*.\n",
    "\n",
    "The ***median*** is almost always a good choice since it is not sensitive to outliers. <br/>\n",
    "However, let us see the distribution of the `total_bedrooms` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 4))\n",
    "\n",
    "sns.stripplot(data=housing_train, x='total_bedrooms', ax=axes[0], alpha=0.2)\n",
    "sns.boxplot(data=housing_train, x='total_bedrooms', ax=axes[1])\n",
    "sns.violinplot(data=housing_train, x='total_bedrooms', ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-clear",
   "metadata": {},
   "source": [
    "We have many ***outliers*** for the `total_bedrooms`. However, these outliers seem to be correct ‚Äî not provided by acquisition errors ‚Äî since there are many densely populated districts in California (e.g., within Los Angeles), which, consequently, results in more houses and bedrooms in the district. <br/>\n",
    "\n",
    "Therefore, we will fill in the missing `total_bedrooms` with the <b style='color: red'>median</b> **`total_bedrooms`** of the _training set_. This same median should be used to fill in missing `total_bedrooms` in the **testing set** ‚û°Ô∏è no ***data snooping bias***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-interaction",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>Using the <b>median <code>total_bedrooms</code></b> of the <b>entire</b> <i>training set</i> is an initial strategy that may be enough to solve this problem.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>However, note that the number of bedrooms of small districts (with a small population) will be considered when setting the median for samples from big districts and vice-versa.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>The same problem would occur with other features: e.g., the income from rich districts would be considered for filling missing incomes from poor districts (for using all samples to compute the median).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>One better alternative could be <i>to group the districts according to their <b>location</b></i> since neighboring districts tend to have <i>similar characteristics.</i></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>Then, we could compute a <i>median for each group</i>. To fill in the missing value of a given new sample, we would select the median from its group.</td>\n",
    "</tr>\n",
    "</table><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-nature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-firmware",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>We need to save this <b>median value</b> to use it in the <i>testing set</i>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>An alternative to <code>.fillna</code> is to use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\"><code>SimpleImputer</code> from sklearn</a>. We will use it in later sprints.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-drive",
   "metadata": {},
   "source": [
    "## üìä 4. Explore the Data\n",
    "We next perform a _simple exploratory data analysis (EDA)_ to gain insights about the data. <br/>\n",
    "A more complete EDA with hypotheses about the problem should be further elaborated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-vitamin",
   "metadata": {},
   "source": [
    "### 4.1. Visualizing Geographical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_clean.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1, figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_clean.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing_train_clean[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"viridis\"), colorbar=True)\n",
    "plt.xlabel('longitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To install plotly\n",
    "# pip install plotly\n",
    "# jupyter labextension install jupyterlab-plotly\n",
    "\n",
    "### Plotly Maps\n",
    "# https://plotly.com/python/scattermapbox/\n",
    "# To plot on Mapbox maps with Plotly you may need a Mapbox account and a public Mapbox Access Token - https://www.mapbox.com/studio\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.set_mapbox_access_token('pk.eyJ1IjoiY2llbmNpYWRlZGFkb3NpZnNwY2FtcGluYXMiLCJhIjoiY2tzcW9sNTRhMGR2bzJ1cGcxNTI1bWppdiJ9.4TJwkUhuLIt-2nH0YudsMg')\n",
    "fig = px.scatter_mapbox(housing_train_clean, lat=\"latitude\", lon=\"longitude\", color=\"median_house_value\", size=\"population\",\n",
    "                        color_continuous_scale=px.colors.sequential.Viridis, size_max=15, zoom=5, width=1000, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-apartment",
   "metadata": {},
   "source": [
    "Some (obvious) findings:\n",
    "- There small and big districts (in term of population) close and far from the coast\n",
    "- The most expensive houses are very close to the coast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-scheduling",
   "metadata": {},
   "source": [
    "### 4.2. Looking for Correlations\n",
    "The **correlation coefficient** measures the _linear correlation_ between two variables. It ranges from ‚Äì1 (perfect negative correlation)  to 1 (perfect positive correlation).\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/2560px-Correlation_examples2.svg.png' width=800>\n",
    "\n",
    "Source: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/2560px-Correlation_examples2.svg.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 numeric attributes ==> 9 x 9 = 81 plots\n",
    "sns.pairplot(data=housing_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the correlation coeficients as a heatmap\n",
    "plt.figure(figsize=(16, 6))\n",
    "mask = np.triu(np.ones_like(housing_train_clean.corr(), dtype=np.bool))  # creates a triangular matrix based on the pandas correlation matrix\n",
    "\n",
    "heatmap = sns.heatmap(housing_train_clean.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_clean.corr()[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-deputy",
   "metadata": {},
   "source": [
    "The _most promising attribute_ to predict the `median house value` is the **`median income`**, so let‚Äôs zoom in on their correlation scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.lmplot(data=housing_train_clean, x=\"median_income\", y=\"median_house_value\", aspect=2, height=8, scatter_kws={'alpha': 0.3}, line_kws={'color': 'r'})\n",
    "plt.yticks(range(0, 700001, 25000))\n",
    "plt.grid(True)\n",
    "plt.title('Median income vs Median house value')\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-aerospace",
   "metadata": {},
   "source": [
    "Some findings:\n",
    "- The _correlation_ is **strong**: see the regression line and the _upward trend_\n",
    "- The prices are **capped** at \\\\$500,000: see the  horizontal line at \\\\$500,000.\n",
    "- There is a horizontal line around \\\\$350,000, another around \\\\$450,000, perhaps a few more.\n",
    "  - You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.\n",
    "  - For now, we will not remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-constitution",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>By looking again at the correlation plots, we can see this <i>capping</i> phenomenon happening in other attributes (e.g., <code>housing_median_age</code>).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>You could better analyze these cases and, maybe, remove them.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-belly",
   "metadata": {},
   "source": [
    "### 4.3. Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_attributes = housing_train_clean.columns.drop('ocean_proximity')\n",
    "numeric_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numeric_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(20, 10))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "for i, attr in enumerate(numeric_attributes):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    sns.boxplot(data=housing_train_clean, x=attr, ax=axes[row, col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-jason",
   "metadata": {},
   "source": [
    "We can see that 6 attributes have many _outliers_. However, we **will not** remove them because _they do not seem to be noise_.\n",
    "\n",
    "For example, a large and populous city (e.g., San Francisco) has several districts, with several houses and, consequently, many rooms and bedrooms. We can see the correlation between 'population' and these attributes. <br/>\n",
    "Also, some cities in California have big markets, such as cinema and IT, where the incomes tend to be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-inventory",
   "metadata": {},
   "source": [
    "Let's visualize the location of the districts with _outliers_ for the `median income` (those >= 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.set_mapbox_access_token('pk.eyJ1IjoiY2llbmNpYWRlZGFkb3NpZnNwY2FtcGluYXMiLCJhIjoiY2tzcW9sNTRhMGR2bzJ1cGcxNTI1bWppdiJ9.4TJwkUhuLIt-2nH0YudsMg')\n",
    "\n",
    "fig = px.scatter_mapbox(housing_train_clean[housing_train_clean['median_income'] >= 8], lat=\"latitude\", lon=\"longitude\", color=\"median_house_value\", size=\"population\",\n",
    "                        color_continuous_scale=px.colors.sequential.Viridis, size_max=15, zoom=5, width=1000, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in the stats for the target outcome\n",
    "housing_train_clean['median_house_value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-pathology",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-vertical",
   "metadata": {},
   "source": [
    "#### **Separating the independent variables (features) and the _dependent variable_ (target outcome)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-finish",
   "metadata": {},
   "source": [
    "Since we do not necessarily want to apply the same transformations to the features and the target outcome, let's separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre = housing_train_clean.drop(columns=['median_house_value'])\n",
    "housing_train_target = housing_train_clean['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-briefs",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>This separation could also happen before <i>data cleaning</i> to isolate the transformations only on the <i>training features.</i></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>However, there are some concerns. First, if you have samples with <i>missing values</i> for any <i>feature</i> and you decide <b>to drop</b> these samples, you <b>must drop</b> the corresponding ones in the <i>target outcome</i> to keep the number of instances consistent, and vice-versa.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-chemical",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>Remember to merge the <i>features</i> and the <i>target outcome</i> into a single dataframe before saving it to disk.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-ribbon",
   "metadata": {},
   "source": [
    "### 5.1. Categorical Variabel Encoding\n",
    "We have a single _categorical variable_ (`ocean_proximity`) to encode. For that, let's use the [**One Hot Encoding** strategy](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02): a new binary variable/column (**dummy variable**) is added for each _category_ from the variable.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding by pandas\n",
    "housing_train_pre = pd.get_dummies(data=housing_train_pre, columns=['ocean_proximity'])\n",
    "housing_train_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-republican",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>To work properly, we should perform the method <code>.get_dummies()</code> for the <b>entire dataset</b>, and <i>not only</i> for the <b>training set</b>. The reason is that this function will create a new column (<i>dummy variable</i>) for each value in the target categorical variable.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>If the categorical variable contains <b>a different number of values</b> in the <i>training set</i> and <i>testing set</i>, the encoding <b>will not be consistent:</b> the number of dummy variables will be different.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>An alternative is to use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\"><code>OneHotEncoder</code> from sklearn</a>. We will use it in the later sprints.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-james",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>When incorporating <b>dummy variables</b> in <i><b>regression</b> algorithms</i>, we should be careful with the <b><i>dummy variable trap</i></b>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>The <b><i>dummy variable trap</i></b> occurs when <i>two or more dummy variables</i> created by one-hot encoding are <i>highly correlated</i> (<b>multi-collinear</b>). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models (<a href=\"https://www.learndatasci.com/glossary/dummy-variable-trap/#:~:text=machine%20learning%20courses.-,What%20is%20the%20Dummy%20Variable%20Trap%3F,coefficient%20variables%20in%20regression%20models.\">source</a>).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>To overcome the dummy variable trap, we just <b>drop one</b> of the <i>dummy variables (column)</i>: <code>.get_dummies(drop_first=True)</code>. This can be done because the <i>dummy variables</i> include <i>redundant information</i>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>Apparently, most methods in sklearn deal with this problem, so we need to do nothing (<i>not sure about it</i>).</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-spiritual",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>If a categorical attribute has a <b>large number of possible categories</b> (e.g., country code, profession, species), then <i>one-hot encoding</i> will result in <i>a large number of input features</i>. This may slow down training and degrade performance.<br/> If this happens, we have some alternatives:</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td><b>1)</b> Replace the <b>categorical input</b> with useful <i><b>numerical features related to the categories</b></i>. <br/> For example, you could replace the <code>ocean_proximity</code> feature with the <i>distance to the ocean</i>.<br/> Similarly, a <code>country code</code> could be replaced with the <i>country‚Äôs population and GDP per capita</i>).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td><b>3)</b> Create <i>another categorical variable</i> that <b>groups the target categories</b> into <i>high-level ones</i>. <br/> For example, you could use consider the continent instead of the country.<br/>However, the higher the abstraction, the greater the loss of information of the original categorical variable.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td><b>3)</b> Replace each category with a learnable, low-dimensional vector called an <i>embedding</i>. <br/> Each category‚Äôs representation would be learned during training. <br/> This is an example of <i>representation learning</i>.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-pixel",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>We can evaluate different tasks for <i>data cleaning</i> and/or <i>data preparation</i> and check <b>which are the best (or the best combination)</b> for our data.</td>        \n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>For this purpose, we treat theses tasks as <i>hyperparameters</i> and <b>fine-tune them / hyperparameter optimization</b> (e.g., <i>grid search</i>). <br/> We may <i>automatically</i> find the best way to handle outliers, missing features, feature selection, etc.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>We will study and use fine-tunning approaches in later sprints.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-seminar",
   "metadata": {},
   "source": [
    "#### **Saving the pre-processed training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre_saving = housing_train_pre.copy()\n",
    "housing_train_pre_saving['median_house_value'] = housing_train_target\n",
    "housing_train_pre_saving.to_csv('./datasets/housing_train_pre_sprint-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-grill",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è 6. Train ML Algorithms\n",
    "In this step, we should train _many_ quick-and-dirty models ‚Äî with _default values_ for their hyperparameters ‚Äî from different categories. <br/>\n",
    "We then _measure the performance_ on the **training set** and shortlist the _top three to five_ **most promising models**.\n",
    "\n",
    "However, we will only select _one model_ in this sprint ‚Äî **Linear Regression** ‚Äî for teaching purposes. We will try new ones in later sprints.\n",
    "\n",
    "<img src='./imgs/assumptions-of-linear-regression.png' width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-stick",
   "metadata": {},
   "source": [
    "### 6.1. Getting the independent (features) and dependent variables (outcome)\n",
    "Before training our models, we want to convert the DataFrame and Series for numpy arrays. <br/>\n",
    "This is not necessarily on this case since the sklearn models can deal with pandas structures.\n",
    "\n",
    "However, since we will use sklearn transformers in the future, whose outputs will be numpy arrays, let's consider numpy right now.\n",
    "\n",
    "Just to keep the standard notation of the ML community, let's use **X** for numpy *feature matrix* and **y** for a numpy array of *labels/outcomes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "enabling-filename",
   "metadata": {},
   "source": [
    "### 6.1. Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-facility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "thousand-senegal",
   "metadata": {},
   "source": [
    "### 6.3. Evaluating on the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-louis",
   "metadata": {},
   "source": [
    "### **Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-feedback",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "concerned-extraction",
   "metadata": {},
   "source": [
    "### **Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-excuse",
   "metadata": {},
   "source": [
    "##### **Coefficient of Determination - R¬≤**\n",
    "The proportion of _variation_ in the dependent variable (outcome) that is explained by the predictor variables (regression, in this case). <br/>\n",
    "R¬≤ can be interpreted as the percent of variance in our dependent variable that can be explained by our model. <br/>\n",
    "R¬≤ checks how much the regression line fits the data\n",
    "\n",
    "$$R^2(y, \\hat{y}) = 1 - \\frac {\\sum_{i=0}^{n-1}(y^{(i)}-\\hat{y}^{(i)})^2}{\\sum_{i=0}^{m-1}(y^{(i)}-\\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-berry",
   "metadata": {},
   "source": [
    "##### **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "$$RMSE = \\sqrt{MSE} = \\sqrt{\\frac{\\sum_{i=0}^{m-1}(y^{(i)}-\\hat{y}^{(i)})^2}{m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-circumstances",
   "metadata": {},
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-effort",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-carol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wound-ireland",
   "metadata": {},
   "source": [
    "This initial result is not too bad, but clearly not a great score: most districts‚Äô `median_housing_values` approximately range between \\\\$120,000 (Q1) and $265,000 (Q3), so a typical **prediction error** of \\\\$69,050 may not be very satisfying. <br/>\n",
    "\n",
    "This is an example of a **model *underfitting*** the training data. When this happens it can mean that the _features_ **do not** provide enough information to make good predictions, or that the model is not powerful enough.\n",
    "\n",
    "<img src='./imgs/overfitting-vs-underfitting.png' width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-broadcasting",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>The main ways <b>to fix <i>underfitting</i></b> are: <br/>\n",
    "        - To select a more powerful model;<br/>\n",
    "        - To feed the training algorithm with better features; or <br/>\n",
    "        - To reduce the constraints on the model (foregularized models).</td>        \n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-staff",
   "metadata": {},
   "source": [
    "### **Visual Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-explanation",
   "metadata": {},
   "source": [
    "##### **Prediction vs Real**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train_pred, y=y_train)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Real')\n",
    "plt.title('Median housing value - Prediction vs Real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-harmony",
   "metadata": {},
   "source": [
    "##### **Residual Analysis**\n",
    "Plot of Prediction vs Residual. This analysis is interesting because we can detect if we meet the assumption of **homoscedasticity**.\n",
    "\n",
    "<img src='./imgs/residual-analysis.png' width=600/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train_pred, y=residual)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Median housing value - Prediction vs Residual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-diamond",
   "metadata": {},
   "source": [
    "Our model **does not** meet the **homoscedasticity**. It seems that the residual is correlated to the prediction values (see the trend). <br/>\n",
    "Also, see strange behavior with the *top diagonal*. It is very likely to be caused by the capped house ages and prices. We need to treat them.\n",
    "\n",
    "When violating _homoscedasticity_, you may want to do some work on your input data:\n",
    "- Maybe you have some variables to add or remove;\n",
    "- Another solution is to do transformations (e.g,. logistic or square root transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-treasure",
   "metadata": {},
   "source": [
    "The residual roghly follows a _normal distribution_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-bicycle",
   "metadata": {},
   "source": [
    "## Some strategies to improve the model\n",
    "- Adding or remove features\n",
    "- Cleaning up outliers\n",
    "- Apply transformations\n",
    "- Try other models/algorithms (mainly nonlinear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
